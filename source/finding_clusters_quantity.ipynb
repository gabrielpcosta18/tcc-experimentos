{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from numpy import *\n",
    "\n",
    "#Nice graphing tools\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.tools as tls\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.cluster.hierarchy import cophenet, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "PREPROCESSED_PATH = './../preprocessed/'\n",
    "IMAGES_PATH = './../images/'\n",
    "\n",
    "FOR_TEST_PREDICTION = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading groupped data\n",
    "def read_groupped_data(filename):\n",
    "    timeSeries = pd.read_csv(filename,index_col=['Unnamed: 0'], parse_dates=['Unnamed: 0'])\n",
    "    timeSeries.rename(columns=lambda x: eval(x), inplace=True)\n",
    "    timeSeries.index = pd.to_datetime(timeSeries.index)\n",
    "    return timeSeries\n",
    "\n",
    "# Plot dendrogram\n",
    "def plot_dendrogram(Z, n_clusters):\n",
    "    plt.figure(figsize=(40, 16))\n",
    "    plt.title('Cluster Dendogram')\n",
    "    plt.xlabel('Timeseries')\n",
    "    plt.ylabel('Distance')\n",
    "    hac.dendrogram(\n",
    "        Z,\n",
    "        truncate_mode='lastp',\n",
    "        p=n_clusters,\n",
    "        show_leaf_counts=True,\n",
    "        show_contracted=True,\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=8.,  # font size for the x axis labels\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "def plot_silhouette(timeSeries, silhouette_avg, n_clusters, cluster_labels, pic_name=None):\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(18, 18)\n",
    "\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "\n",
    "    ax1.set_ylim([0, len(timeSeries) + (n_clusters + 3) * 10])\n",
    "    y_lower = 10\n",
    "    \n",
    "    sample_silhouette_values = metrics.silhouette_samples(timeSeries, cluster_labels)\n",
    "    silhouette_values = []\n",
    "    \n",
    "    for i in range(0, cluster_labels.max() + 1):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "        \n",
    "        if ith_cluster_silhouette_values.shape[0] > 0:\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "            silhouette_values.append(ith_cluster_silhouette_values)\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        else:\n",
    "            silhouette_values.append(pd.Series([]))\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-1.0, -0.9,  -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    if pic_name:\n",
    "        fig.savefig(IMAGES_PATH + pic_name + '.png')\n",
    "    plt.suptitle((\"Silhouette analysis for Time Series Clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    return pd.Series(silhouette_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading time series data\n",
    "timeSeries = read_groupped_data(PREPROCESSED_PATH + 'timeseries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing time series without data\n",
    "timeSeries = timeSeries.dropna(axis=1)\n",
    "timeSeries = timeSeries.iloc[:-FOR_TEST_PREDICTION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization between 0-1\n",
    "normalization = MinMaxScaler()\n",
    "\n",
    "# Z-score normalization\n",
    "standardization = StandardScaler()\n",
    "\n",
    "# Normalizing time series\n",
    "timeSeriesNormalized = pd.DataFrame(normalization.fit_transform(timeSeries), index=timeSeries.index, columns=timeSeries.columns)\n",
    "timeSeriesStandardized = pd.DataFrame(standardization.fit_transform(timeSeries), index=timeSeries.index, columns=timeSeries.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = timeSeriesNormalized.T\n",
    "X_standardized = timeSeriesStandardized.T\n",
    "\n",
    "# Applying ward method for clustering\n",
    "Z_normalized = hac.linkage(X_normalized, method='ward')\n",
    "Z_standardized = hac.linkage(X_standardized, method='ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized:  2 0.24759562281867598\n",
      "Normalized:  3 0.18559553252605657\n",
      "Normalized:  4 0.0990930217342644\n",
      "Normalized:  5 0.1122187226553964\n",
      "Normalized:  6 0.10743503378849065\n",
      "Normalized:  7 0.11013020803890006\n",
      "Normalized:  8 0.10795684484505406\n",
      "Normalized:  9 0.11490404584621292\n",
      "Normalized:  10 0.11389880585714272\n",
      "Normalized:  11 0.11246892863454214\n",
      "Normalized:  12 0.10780656844560443\n",
      "Normalized:  13 0.1097336136540592\n",
      "Normalized:  14 0.11651662248401999\n",
      "Normalized:  15 0.0713159361703258\n",
      "Normalized:  16 0.07253649274482671\n",
      "Normalized:  17 0.0755587919709479\n",
      "Normalized:  18 0.07552951221807223\n",
      "Normalized:  19 0.07682747039982014\n",
      "Normalized:  20 0.07763008002442073\n",
      "Normalized:  21 0.08022013140326474\n",
      "Normalized:  22 0.0743465734263002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py:522: RuntimeWarning:\n",
      "\n",
      "More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized:  23 0.07451766976108919\n",
      "Normalized:  24 0.07654326517028637\n",
      "Normalized:  25 0.07831812604310738\n",
      "Normalized:  26 0.07762513633520701\n",
      "Normalized:  27 0.07960932836593002\n",
      "Normalized:  28 0.08009008916841286\n",
      "Normalized:  29 0.07457801889917927\n",
      "Normalized:  30 0.07545010919885078\n",
      "Normalized:  31 0.07719461231021071\n",
      "Normalized:  32 0.07662269214184139\n",
      "Normalized:  33 0.07701681508015588\n",
      "Normalized:  34 0.07819470521532056\n",
      "Normalized:  35 0.07931862711392082\n",
      "Normalized:  36 0.0801378087274541\n",
      "Normalized:  37 0.08018581840803593\n",
      "Normalized:  38 0.0795739154776869\n",
      "Normalized:  39 0.07999118490097151\n",
      "Normalized:  40 0.07937414183055136\n",
      "Normalized:  41 0.07831690622353038\n",
      "Normalized:  42 0.07803572066739227\n",
      "Normalized:  43 0.07874197968185057\n",
      "Normalized:  44 0.0782232672800176\n",
      "Normalized:  45 0.0800422011703614\n",
      "Normalized:  46 0.08062974673358053\n",
      "Normalized:  47 0.08054860678181558\n",
      "Normalized:  48 0.07991436101530068\n",
      "Normalized:  49 0.0759695753583281\n",
      "Normalized:  50 0.07735397301175721\n",
      "Normalized:  51 0.07772574246301321\n",
      "Normalized:  52 0.07791758447515094\n",
      "Normalized:  53 0.07832131340939576\n",
      "Normalized:  54 0.07837837471510566\n",
      "Normalized:  55 0.07941101855235688\n",
      "Normalized:  56 0.08000605875622413\n",
      "Normalized:  57 0.08038624581869479\n",
      "Normalized:  58 0.0811065789691262\n",
      "Normalized:  59 0.0821011878807376\n",
      "Normalized:  60 0.08301388318008389\n",
      "Normalized:  61 0.08313753063902347\n",
      "Normalized:  62 0.08375897899065221\n",
      "Normalized:  63 0.08284217086926973\n",
      "Normalized:  64 0.08298464672970708\n",
      "Normalized:  65 0.08427453413308425\n",
      "Normalized:  66 0.08498381283547993\n",
      "Normalized:  67 0.08474708649537238\n",
      "Normalized:  68 0.08551031030088072\n",
      "Normalized:  69 0.08464151456503892\n",
      "Normalized:  70 0.08473522722496259\n",
      "Normalized:  71 0.08499528145106551\n",
      "Normalized:  72 0.08453397704544058\n",
      "Normalized:  73 0.08498413825516522\n",
      "Normalized:  74 0.08492364024822457\n",
      "Normalized:  75 0.08484625460210948\n",
      "Normalized:  76 0.08503666683117052\n",
      "Normalized:  77 0.08564754063923322\n",
      "Normalized:  78 0.08611842813546111\n",
      "Normalized:  79 0.0866125001465584\n",
      "Normalized:  80 0.0849910189513897\n",
      "Normalized:  81 0.08511535218739828\n",
      "Normalized:  82 0.08452637285751431\n",
      "Normalized:  83 0.08561038247656653\n",
      "Normalized:  84 0.08609027703300229\n",
      "Normalized:  85 0.0866481191071578\n",
      "Normalized:  86 0.08676807213874466\n",
      "Normalized:  87 0.08645587624314754\n",
      "Normalized:  88 0.08656632171856636\n",
      "Normalized:  89 0.08662358317743277\n",
      "Normalized:  90 0.08830268269265594\n",
      "Normalized:  91 0.08807515511072875\n",
      "Normalized:  92 0.08685175262177372\n",
      "Normalized:  93 0.08695172611022911\n",
      "Normalized:  94 0.08743314412498823\n",
      "Normalized:  95 0.08804854200140674\n",
      "Normalized:  96 0.08816900570500279\n",
      "Normalized:  97 0.08853390926347736\n",
      "Normalized:  98 0.08932871346056295\n",
      "Normalized:  99 0.08965909762705045\n"
     ]
    }
   ],
   "source": [
    "TEST_SILHOUETTE_AVG = True\n",
    "        \n",
    "if TEST_SILHOUETTE_AVG:\n",
    "    normalized_avg = pd.DataFrame(columns=['n_clusters', 'avg'])\n",
    "    normalized_min = pd.DataFrame(columns=['n_clusters', 'avg'])\n",
    "    normalized_max = pd.DataFrame(columns=['n_clusters', 'avg'])\n",
    "    \n",
    "    standardized_avg = pd.DataFrame(columns=['n_clusters', 'avg'])\n",
    "    standardized_min = pd.DataFrame(columns=['n_clusters', 'avg'])\n",
    "    standardized_max = pd.DataFrame(columns=['n_clusters', 'avg'])\n",
    "    for i in range(2, 100):\n",
    "        cluster_labels = fcluster(Z_normalized, i, criterion=\"maxclust\")\n",
    "        avg = metrics.silhouette_score(X_normalized, cluster_labels, metric='euclidean')\n",
    "        samples = pd.DataFrame({'cluster_labels': cluster_labels, 'coefficiente': metrics.silhouette_samples(X_normalized, cluster_labels)}).groupby(['cluster_labels'])\n",
    "        \n",
    "#         from_dendrogram = plot_silhouette(X_normalized, avg, i, cluster_labels)\n",
    "\n",
    "        \n",
    "        normalized_avg = normalized_avg.append(pd.DataFrame([[i, avg]], columns=['n_clusters', 'avg']), ignore_index=True)\n",
    "        normalized_min = normalized_min.append(pd.DataFrame([[i, samples.mean().min()]], columns=['n_clusters', 'avg']), ignore_index=True)\n",
    "        normalized_max = normalized_max.append(pd.DataFrame([[i, samples.mean().max()]], columns=['n_clusters', 'avg']), ignore_index=True)\n",
    "        \n",
    "        cluster_labels = fcluster(Z_standardized, i, criterion=\"maxclust\")\n",
    "        avg = metrics.silhouette_score(X_standardized, cluster_labels, metric='euclidean')\n",
    "        samples = pd.DataFrame({'cluster_labels': cluster_labels, 'coefficiente': metrics.silhouette_samples(X_standardized, cluster_labels)}).groupby(['cluster_labels'])\n",
    "        \n",
    "        standardized_avg = standardized_avg.append(pd.DataFrame([[i, avg]], columns=['n_clusters', 'avg']), ignore_index=True)\n",
    "        standardized_min = standardized_min.append(pd.DataFrame([[i, samples.mean().min()]], columns=['n_clusters', 'avg']), ignore_index=True)\n",
    "        standardized_max = standardized_max.append(pd.DataFrame([[i, samples.mean().max()]], columns=['n_clusters', 'avg']), ignore_index=True)\n",
    "        \n",
    "    plt.figure(figsize=(30,16,))\n",
    "    plt.title('Avaliando quantidade de clusters com coeficiente de silhueta', fontsize=35)\n",
    "    plt.xlabel('Quantidade de clusters', fontsize=30)\n",
    "    plt.ylabel('Coeficiente de silhueta', fontsize=30)\n",
    "    plt.plot(normalized_avg.set_index('n_clusters'), label='Coeficiente médio')\n",
    "    plt.plot(normalized_min.set_index('n_clusters'), label='Coeficiente mínimo')\n",
    "    plt.plot(normalized_max.set_index('n_clusters'), label='Coeficiente máximo')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.legend(loc='best', prop={'size': 30})\n",
    "    plt.savefig(IMAGES_PATH + 'avg_min_max_normalization.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(30,16,))\n",
    "    plt.title('Avaliando quantidade de clusters com coeficiente de silhueta', fontsize=35)\n",
    "    plt.xlabel('Quantidade de clusters', fontsize=30)\n",
    "    plt.ylabel('Coeficiente de silhueta', fontsize=30)\n",
    "    plt.plot(standardized_avg.set_index('n_clusters'), label='Coeficiente médio')\n",
    "    plt.plot(standardized_min.set_index('n_clusters'), label='Coeficiente mínimo')\n",
    "    plt.plot(standardized_max.set_index('n_clusters'), label='Coeficiente máximo')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.legend(loc='best', prop={'size': 30})\n",
    "    plt.savefig(IMAGES_PATH + 'avg_min_max_standardization.png')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(30,16,))\n",
    "    plt.title('Avaliando quantidade de clusters com coeficiente de silhueta', fontsize=35)\n",
    "    plt.xlabel('Quantidade de clusters', fontsize=30)\n",
    "    plt.ylabel('Coeficiente de silhueta', fontsize=30)\n",
    "    plt.plot(normalized_avg.set_index('n_clusters')[1:], label='Coeficiente médio com séries normalizadas')\n",
    "    plt.plot(standardized_avg.set_index('n_clusters')[1:], label='Coeficiente médio com séries z-normalizadas')\n",
    "    plt.tick_params(axis='both', which='major', labelsize=20)\n",
    "    plt.legend(loc='best', prop={'size': 30})\n",
    "    plt.savefig(IMAGES_PATH + 'avaliacao_coef_silhueta.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\python.exe\n",
      "Collecting tslearn\n",
      "  Using cached https://files.pythonhosted.org/packages/95/da/eccb7d5a21bc38d77e99101e4fd6c0044a9fea7f4e254ac1c6ec209db29c/tslearn-0.1.24.tar.gz\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from tslearn) (1.15.1)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from tslearn) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from tslearn) (0.19.2)\n",
      "Requirement already satisfied: Cython in c:\\programdata\\anaconda3\\lib\\site-packages (from tslearn) (0.28.5)\n",
      "Building wheels for collected packages: tslearn\n",
      "  Running setup.py bdist_wheel for tslearn: started\n",
      "  Running setup.py bdist_wheel for tslearn: still running...\n",
      "  Running setup.py bdist_wheel for tslearn: still running...\n",
      "  Running setup.py bdist_wheel for tslearn: still running...\n",
      "  Running setup.py bdist_wheel for tslearn: still running...\n",
      "  Running setup.py bdist_wheel for tslearn: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\gabri\\AppData\\Local\\pip\\Cache\\wheels\\23\\a3\\65\\51f5e2defdf863b80c4cfdb8714882e08193d3eedf011f1351\n",
      "Successfully built tslearn\n",
      "Installing collected packages: tslearn\n",
      "Successfully installed tslearn-0.1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "twisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\n",
      "You are using pip version 10.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# X_normalized.head()\n",
    "# import tslearn.clustering.silhouette_score\n",
    "# cluster_labels = fcluster(Z_normalized, 45, criterion=\"maxclust\")\n",
    "# silhouette_score(X_normalized, cluster_labels, metric=\"dtw\")\n",
    "\n",
    "import sys \n",
    "print(sys.executable)\n",
    "!{sys.executable} -m pip install tslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.metrics import cdist_dtw\n",
    "resultado = cdist_dtw(X_standardized.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
